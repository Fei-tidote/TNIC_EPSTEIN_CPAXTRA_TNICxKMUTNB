{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fei-tidote/TNIC_EPSTEIN_CPAXTRA_TNICxKMUTNB/blob/main/TNIC_EPSTEIN_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "def build_dataset():\n",
        "    print(\"Loading datasets...\")\n",
        "    sales = pd.read_csv('/content/sales_history.csv')\n",
        "    price_cost = pd.read_csv('/content/price_cost.csv')\n",
        "    inv = pd.read_csv('/content/inventory.csv')\n",
        "    comp = pd.read_csv('/content/competitor_prices.csv')\n",
        "    xel = pd.read_csv('/content/XEL.csv')\n",
        "    cal = pd.read_csv('/content/calendar_weather.csv')\n",
        "    sku = pd.read_csv('/content/sku_master.csv')\n",
        "    store = pd.read_csv('/content/store_master.csv')\n",
        "\n",
        "    sales['date'] = pd.to_datetime(sales['date'])\n",
        "    cal['date'] = pd.to_datetime(cal['date'])\n",
        "    comp['date'] = pd.to_datetime(comp['date'])\n",
        "\n",
        "    top_subs = xel.loc[xel.groupby('sku_i')['xel_ij'].idxmin()].rename(columns={'sku_i': 'sku_id', 'sku_j': 'sub_sku_id'})\n",
        "    sub_prices = comp.rename(columns={'sku_id': 'sub_sku_id', 'comp_price': 'sub_comp_price'})\n",
        "\n",
        "    master = sales.merge(price_cost, on='sku_id', how='left')\n",
        "    master = master.merge(cal, on='date', how='left')\n",
        "    master = master.merge(sku, on='sku_id', how='left')\n",
        "    master = master.merge(store, on='store_id', how='left')\n",
        "\n",
        "    master = master.merge(comp, on=['sku_id', 'date'], how='left')\n",
        "    master = master.merge(top_subs[['sku_id', 'sub_sku_id']], on='sku_id', how='left')\n",
        "    master = master.merge(sub_prices, on=['sub_sku_id', 'date'], how='left')\n",
        "\n",
        "    master['comp_price'] = master['comp_price'].fillna(master['regular_price'])\n",
        "    master['sub_comp_price'] = master['sub_comp_price'].fillna(master['regular_price'])\n",
        "\n",
        "    return master, price_cost, inv, cal, comp, top_subs, sku, store\n",
        "\n",
        "def generate_price():\n",
        "    master, price_cost, inv, cal, comp, top_subs, sku, store = build_dataset()\n",
        "\n",
        "    numeric_features = [\n",
        "        'dow', 'is_payday', 'is_holiday', 'temp', 'rain_index',\n",
        "        'regular_price', 'unit_cost', 'comp_price', 'sub_comp_price',\n",
        "        'price_paid', 'income_index'\n",
        "    ]\n",
        "\n",
        "    categorical_features = [\n",
        "        'store_id', 'sku_id', 'category', 'subcategory', 'brand', 'region', 'store_type'\n",
        "    ]\n",
        "\n",
        "    for col in categorical_features:\n",
        "        master[col] = master[col].astype('category')\n",
        "\n",
        "    all_features = numeric_features + categorical_features\n",
        "    train_df = master.dropna(subset=all_features + ['qty']).copy()\n",
        "\n",
        "    print(\"Training Identity-Aware LightGBM Model...\")\n",
        "    model = LGBMRegressor(n_estimators=250, learning_rate=0.05, max_depth=12, random_state=42)\n",
        "    model.fit(train_df[all_features], train_df['qty'], categorical_feature=categorical_features)\n",
        "\n",
        "    print(\"Executing...\")\n",
        "    sub = pd.read_csv('sample_submission.csv')\n",
        "    sub['date_parsed'] = pd.to_datetime(sub['date'], format='%d/%m/%Y')\n",
        "    cal_future = cal[cal['date'].isin(sub['date_parsed'].unique())].copy()\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for (store_id, sku_id), group in sub.groupby(['store_id', 'sku_id']):\n",
        "        pc = price_cost[price_cost['sku_id'] == sku_id].iloc[0]\n",
        "        inv_record = inv[(inv['store_id'] == store_id) & (inv['sku_id'] == sku_id)]\n",
        "\n",
        "        unit_cost = pc['unit_cost']\n",
        "        vat_rate = pc['vat_rate']\n",
        "        regular_price = pc['regular_price']\n",
        "        current_inventory = inv_record['on_hand'].iloc[0] if not inv_record.empty else 9999\n",
        "\n",
        "        group_dates = group['date_parsed']\n",
        "        future_df = cal_future[cal_future['date'].isin(group_dates)].copy()\n",
        "\n",
        "        future_df['store_id'] = store_id\n",
        "        future_df['sku_id'] = sku_id\n",
        "        future_df = future_df.merge(sku, on='sku_id', how='left')\n",
        "        future_df = future_df.merge(store, on='store_id', how='left')\n",
        "\n",
        "        sub_id = top_subs[top_subs['sku_id'] == sku_id]['sub_sku_id'].values\n",
        "        sub_sku = sub_id[0] if len(sub_id) > 0 else sku_id\n",
        "\n",
        "        comp_data = comp[(comp['sku_id'] == sku_id) & (comp['date'].isin(group_dates))]\n",
        "        sub_comp_data = comp[(comp['sku_id'] == sub_sku) & (comp['date'].isin(group_dates))]\n",
        "\n",
        "        future_df = future_df.merge(comp_data[['date', 'comp_price']], on='date', how='left')\n",
        "        future_df = future_df.merge(sub_comp_data[['date', 'comp_price']].rename(columns={'comp_price': 'sub_comp_price'}), on='date', how='left')\n",
        "        future_df['comp_price'] = future_df['comp_price'].fillna(regular_price)\n",
        "        future_df['sub_comp_price'] = future_df['sub_comp_price'].fillna(regular_price)\n",
        "\n",
        "        for col in categorical_features:\n",
        "            future_df[col] = future_df[col].astype('category')\n",
        "\n",
        "        p_min = max(unit_cost * (1 + vat_rate), regular_price * 0.70)\n",
        "        p_max = regular_price * 1.30\n",
        "\n",
        "        candidate_prices = [\n",
        "            round(base + ending, 2)\n",
        "            for base in range(int(np.floor(p_min)), int(np.ceil(p_max)) + 1)\n",
        "            for ending in [0.00, 0.50, 0.90]\n",
        "            if p_min <= round(base + ending, 2) <= p_max\n",
        "        ]\n",
        "\n",
        "        if not candidate_prices:\n",
        "            best_price = regular_price\n",
        "        else:\n",
        "            future_df['key'] = 1\n",
        "            prices_df = pd.DataFrame({'price_paid': candidate_prices, 'key': 1})\n",
        "            scoring_grid = pd.merge(future_df, prices_df, on='key').drop('key', axis=1)\n",
        "\n",
        "            scoring_grid['regular_price'] = regular_price\n",
        "            scoring_grid['unit_cost'] = unit_cost\n",
        "\n",
        "            scoring_grid['pred_qty'] = model.predict(scoring_grid[all_features])\n",
        "\n",
        "            results = scoring_grid.groupby('price_paid').agg(total_14d_demand=('pred_qty', 'sum')).reset_index()\n",
        "\n",
        "            results['actual_sales'] = np.where(results['total_14d_demand'] > current_inventory, current_inventory, results['total_14d_demand'])\n",
        "\n",
        "            results['gross_profit'] = ((results['price_paid'] / (1 + vat_rate)) - unit_cost) * results['actual_sales']\n",
        "\n",
        "            results['stockout_penalty'] = np.where(results['total_14d_demand'] > current_inventory, results['total_14d_demand'] - current_inventory, 0)\n",
        "\n",
        "            results['simulated_score'] = results['gross_profit'] - results['stockout_penalty']\n",
        "\n",
        "            best_price = results.loc[results['simulated_score'].idxmax(), 'price_paid']\n",
        "\n",
        "        group_res = group.copy()\n",
        "        group_res['proposed_price'] = best_price\n",
        "        results_list.append(group_res)\n",
        "\n",
        "    final_sub = pd.concat(results_list).drop(columns=['date_parsed']).sort_values('ID')\n",
        "    final_sub.to_csv('submission_final.csv', index=False)\n",
        "    print(\"SUCCESS: submission_final.csv generated!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_price()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KzkX-2cW5Uq",
        "outputId": "a37bd9b3-20ea-4e06-b589-9e39441d53af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Training Identity-Aware LightGBM Model...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014037 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1244\n",
            "[LightGBM] [Info] Number of data points in the train set: 216000, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score 9.926366\n",
            "Executing...\n",
            "SUCCESS: submission_final.csv generated!\n"
          ]
        }
      ]
    }
  ]
}